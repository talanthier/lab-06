---
title: "STAT 108: Lab 6"
author: "Tim Lanthier"
date: "2/24/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 06: Model Selection + Diagnostics

Github Repository: [https://github.com/talanthier/lab-06](https://github.com/talanthier/lab-06)
```{r, message = FALSE}
library(tidyverse)
library(knitr)
library(broom)
library(leaps)
library(rms)
library(Sleuth3) #case1201 data
```

In this lab we will be working with SAT data from the 1982 exam. The dataset can be found in the Sleuth3 package (case 1201).


## Model Selection

We will start with a full linear model including all possible predictor variables and no interaction terms.

```{r}
sat_scores <- Sleuth3::case1201 
full_model <- lm(SAT ~ Takers + Income + Years + Public + Expend + Rank , data = sat_scores)
tidy(full_model) %>% kable(digits = 3)
```
Now we will conduct backwards selection on our full model. We will start by using adjusted $R^2$ as our selection criterion.

```{r}
model_select <- regsubsets(SAT ~ Takers + Income + Years + Public + Expend + 
                             Rank , data = sat_scores, method = "backward")
select_summary <- summary(model_select)
coef(model_select,  which.max(select_summary$adjr2)) # choose model which has highest adj R^2 
```
So using adjusted $R^2$ as our criterion, our best model from our backselected models is the model with `Years`, `Public`, `Expend`, and `Rank` as our predictor variables. 

```{r}
coef(model_select, which.min(select_summary$bic)) # choose model with smallest BIC
```
Meanwhile with BIC, we find the best model to be the one including just `Years`, `Expend`, and `Rank`. Note that for these cases, `regsubsets` is using the residual sum of squares for its criteria for backwards selection. We are only comparing the backwards selected models using Adj. $R^2$ and BIC.

Now we will run backwards selection using AIC as the criteria for eliminating predictors.

```{r}
model_select_aic <- step(full_model, direction = "backward")
```

With AIC as our criterion, backward selection has chosen the model with `Public`, `Expend`, `Years`, and `Rank` as the best model. Comparing the 3 different models we found with different criteria, we see that they don't all match up. While AIC and Adj. $R^2$ agreed on what variables to select, BIC yielded a model with one less variable than the rest. That being said, BIC still selected `Years`, `Expend`, and `Rank` which all appear in the models selected with AIC and Adj. $R^2$. The fact that our BIC selects a model with fewer variables makes sense since BIC tends to penalize models more complex models more heavily than AIC.

## Model Diagnostics

For the remainder of the lab, we will be using our model selected with AIC.

```{r}
model_select_aic_aug <- augment(model_select_aic) %>%
  mutate(obs_num = row_number())

head(model_select_aic_aug,5)
```

Now that we have the model predictions and statistics for each of the observations in our dataset, we will examine the leverage for each observation. Since we're conducting multiple linear regression, leverage has the formula
\[H = X(X^TX)^{-1}X^T\]
where X is the original dataframe with the 4 predictor variables in our model. in `model_select_aic_aug`, leverage for each observation is shown as `.hat`. We will say that an observation has high leverage if it is above the threshold
\[\frac{2(p+1)}{n}\]
where $n$ is the number of observations and $p$ the number of predictors. Hence for our data we have a threshold of
\[\frac{2(4+1)}{50} = 0.2\]

Now we will plot the leverage for all of the observations
```{r}
ggplot(model_select_aic_aug, aes(obs_num, .hat)) +
  geom_point() +
  geom_hline(yintercept = 0.2, color = 'red') +
  labs(y = 'Leverage', x = 'Observation', title = 'Leverage of Observations')
```
Looking at our plot of the leverage, we have 2 clear high leverage observations. We also have 3 states which are close to but not above the threshold.

```{r}
sat_scores[which(model_select_aic_aug$.hat > 0.2), ]
```
So the 2 states which have high leverage are Louisiana and Alaska.

Now we will examine the standard residuals.

```{r}
ggplot(model_select_aic_aug, aes(.fitted, .std.resid)) +
  geom_point() + 
  geom_hline(yintercept = 2, color = 'red') + 
  geom_hline(yintercept = -2, color = 'red') +
  labs(x = 'Predictions', y = 'Standard Residuals', title = 'Standard Residuals vs Predicted Scores')
```
According to the standard residuals, we have 3 states lying outside of our threshold. So 3 states have standard residuals with a large magnitude.

```{r}
sat_scores[which(abs(model_select_aic_aug$.std.resid) > 2), ]
```
As shown above, the 3 states with large standardized residuals are Mississippi, Alaska, and South Carolina.

To check whether these states we identified which have high leverage or high standardized residuals are significantly impacting our model, we will investigate the Cook's distance for our data.

```{r}
ggplot(model_select_aic_aug, aes(obs_num, .cooksd)) +
  geom_point() +
  geom_hline(yintercept = 1, color = 'red') +
  labs(x = 'Observation Number', y = "Cook's Distance")
```
With our threshold of 1 for Cook's distance, we see only one observation lies above our threshold. 
```{r}
sat_scores[which(abs(model_select_aic_aug$.cooksd) > 1), ]
```
Here, that state lying above our threshold is Alaska, which we found to have a high magnitude standardized residual as well as a high magnitude leverage. Seeing as we only have a single influential observation, it might be wise to remove that observation from our dataset. While a model based on this dataset will not be useful in predicting SAT scores in Alaska, removing Alaska might result in predictions for the remaining 49 states to be more accurate. If we wanted to use our model to make predictions on Alaska's SAT scores, we would have to include it in our dataset.

Lastly we need to check for collinearity. For this we will use the Variance Inflation Factor (VIF). We will start by building a model with `Expend` as our response variable and the same predictors as our backward selected model based on AIC.

```{r}
expend_model <- lm(Expend ~ Years + Public + Rank, data = sat_scores)
summary(expend_model)
```
Looking at the above model, we get an $R^2$ of 0.2102. So only approximately 21\% of the variance in `Expend` is explained by `Years`, `Public`, and `Rank`. We can calculate the VIF for `Expend` with the following formula
\[\frac{1}{1-R^2} = \frac{1}{1-0.2102} = 1.266\]
So we have a VIF of 1.266. Since we would consider a VIF of greater than 10 as concerning, there doesn't appear to be any concerning collinearity between `Expend` and the other predictors in the model. Now we will calculate the VIF of the remaining predictors.

```{r}
vif(model_select_aic)
```
The VIF for each of the predictors in our model is shown above. As we can see, the VIFs for the remaining predictors are of similar magnitude to that of `Expend`. So once again, the VIF does not indicate any collinearity between any of our variables. 




